{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a pipeline for comparing the vocabulary of two sets of Tactus emails with eachother by the t-score. The goal is to find tokens which appear more frequently in one set than in the other, and vice versa. This notebook uses much of the preprocessing of the notebook liwc.py in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code block specifies the required libraries. This includes some general Python libraries and some specific libraries developed in our research project. These project-specific libraries can be found in the folder orangehackathon/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append(\"../libs/\")\n",
    "import tactusloaderLIB\n",
    "import OWEmailSorterLIB\n",
    "import markduplicatesLIB\n",
    "import removemarkedtextLIB\n",
    "import LIWCLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block specifies the location of the therapy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"/home/erikt/projects/e-mental-health/usb/releases/20200320\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The therapy files are read with the Orange3 pipeline. The Orange3 pipeline contains these parts:\n",
    "\n",
    "1. tactusloader: determine file name and read its contents\n",
    "2. sortMails: sort the mails from the file chronologically\n",
    "3. markduplicates: mark the parts of the mail text included from an earlier mail\n",
    "4. removemarkedtext: remove the marked text from the mail\n",
    "5. LIWC: analyse the text with LIWC\n",
    "\n",
    "The file loading takes several minutes. The program counts from 1 to the number of files to indicate its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXCLIENT = 1987\n",
    "\n",
    "def readTactusData(maxClient):\n",
    "    allLiwcResults = {}\n",
    "    allMails = {}\n",
    "    emptyFiles = []\n",
    "    problemFiles = []\n",
    "    for patientId in range(1,maxClient+1):\n",
    "        clear_output(wait=True)\n",
    "        print(\"processing:\",patientId)\n",
    "        fileName = tactusloaderLIB.makeFileName(str(patientId))\n",
    "        fileNameId = re.sub(\"-an.xml$\",\"\",fileName)\n",
    "        try:\n",
    "            mails = tactusloaderLIB.processFile(DIRECTORY,fileName+\".gz\")\n",
    "            if len(mails[0]) > 0:\n",
    "                sortedMails = OWEmailSorterLIB.filterEmails(mails[0],filter_asc=True)\n",
    "                markedMails = markduplicatesLIB.processCorpus(sortedMails)\n",
    "                strippedMails = removemarkedtextLIB.processCorpus(markedMails)\n",
    "                liwcResults = LIWCLIB.processCorpus(strippedMails)\n",
    "                allLiwcResults[fileNameId] = liwcResults\n",
    "                allMails[fileNameId] = strippedMails\n",
    "            else: emptyFiles.append(fileName)\n",
    "        except:\n",
    "            problemFiles.append(fileName)\n",
    "            continue\n",
    "    if len(emptyFiles) > 0:\n",
    "        print(\"Found empty or nonexistant files:\",emptyFiles)\n",
    "    if len(problemFiles) > 0:\n",
    "        print(\"There were problems processing these files:\",problemFiles)\n",
    "    return(allLiwcResults,allMails)\n",
    "\n",
    "allLiwcResults,allMails = readTactusData(MAXCLIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allMails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will comparethe texts in emails from clients that finished the treatment versus clients that dropped out. Thus we need the metadata which specifies the results of the therapy for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "DIRDROPOUT = \"/home/erikt/projects/e-mental-health/usb/releases/20200305/\"\n",
    "FILEDROPOUT = \"selected.csv.gz\"\n",
    "DELIMITER = \",\"\n",
    "FIELDNAMEDROPOUT = \"dropout\"\n",
    "FIELDNAMETEXT = \"text\"\n",
    "FIELDNAMEFILE = \"file\"\n",
    "FIELDNAMEFROM = \"from\"\n",
    "CLIENT = \"CLIENT\"\n",
    "COUNSELOR = \"COUNSELOR\"\n",
    "NBROFCLIENTS = 791\n",
    "CODEDROPOUT = \"1\"\n",
    "CODEFINISHER = \"2\"\n",
    "\n",
    "dropout = {}\n",
    "inFile = gzip.open(DIRDROPOUT+FILEDROPOUT,\"rt\",encoding=\"utf-8\")\n",
    "csvreader = csv.DictReader(inFile,delimiter=DELIMITER)\n",
    "for row in csvreader: dropout[row[FIELDNAMEFILE]] = row[FIELDNAMEDROPOUT]\n",
    "inFile.close()\n",
    "\n",
    "len([x for x in dropout if dropout[x] != \"?\"]) == NBROFCLIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare collections of texts with the t-score. First we will compute the t-scores of various text collections with a function `makeTscoreData`. After this we can compare these t-scores with the function `compareTscoreData`. We rely on the t-score script `/home/erikt/projects/newsgac/fasttext-runs/tscore.py` for making the comparisons. \n",
    "\n",
    "There are two ways for computing the t-scores: count every separate word used by a client or count each word used by a client only once. The texts can be prepared for the second type of counts with the function `removeDuplicateTokensTextArray` which removes all duplicate words from the texts (case-sensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFTOKENS = \"totalFreq\"\n",
    "NBROFTYPES = \"nbrOfWords\"\n",
    "WORDFREQS = \"wordFreqs\"\n",
    "NBROFGROUPS = \"nbrOfGroups\"\n",
    "\n",
    "def normalizeMaxCount(tscoreData,fraction):\n",
    "    tscoreData[MAXCOUNT] = round(tscoreData[MAXCOUNT]*fraction,1)\n",
    "    for word in tscoreData[\"wordFreqs\"]:\n",
    "        tscoreData[\"wordFreqs\"][word] = round(tscoreData[\"wordFreqs\"][word]*fraction,1)\n",
    "    return(tscoreData)\n",
    "\n",
    "def removeEmptyMails(textArrayIn):    \n",
    "    return([text for text in textArrayIn if text != \"\"])\n",
    "\n",
    "def removeDuplicateTokensText(text):\n",
    "    seen = {}\n",
    "    for word in text.split():\n",
    "        if not word in seen: seen[word] = True\n",
    "    return(\" \".join(list(seen.keys())))\n",
    "\n",
    "def removeDuplicateTokensTextArray(textArrayIn):\n",
    "    textArrayOut = []\n",
    "    for text in textArrayIn:\n",
    "        textArrayOut.append(removeDuplicateTokens(text))\n",
    "    return(textArrayOut)\n",
    "\n",
    "def makeTscoreData(textArray):\n",
    "    data = { NBROFTOKENS:0, NBROFTYPES:0, NBROFGROUPS:len(textArray), WORDFREQS:{} }\n",
    "    for text in textArray:\n",
    "        for token in text.split():\n",
    "            data[NBROFTOKENS] += 1\n",
    "            if token in data[WORDFREQS]: \n",
    "                data[WORDFREQS][token] += 1\n",
    "            else:\n",
    "                data[WORDFREQS][token] = 1\n",
    "                data[NBROFTYPES] += 1\n",
    "    return(data)\n",
    "\n",
    "def makeTscoreDataBigrams(textArray):\n",
    "    data = { NBROFTOKENS:0, NBROFTYPES:0, NBROFGROUPS:len(textArray), WORDFREQS:{} }\n",
    "    for text in textArray:\n",
    "        tokens = text.split()\n",
    "        for i in range(1,len(tokens)):\n",
    "            bigram = tokens[i-1]+\" \"+tokens[i]\n",
    "            data[NBROFTOKENS] += 1\n",
    "            if bigram in data[WORDFREQS]: \n",
    "                data[WORDFREQS][bigram] += 1\n",
    "            else:\n",
    "                data[WORDFREQS][bigram] = 1\n",
    "                data[NBROFTYPES] += 1\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/erikt/projects/newsgac/fasttext-runs\")\n",
    "import tscore\n",
    "import operator\n",
    "\n",
    "def compareTscoreData(tscoreData1,tscoreData1reference,tscoreData2,tscoreData2reference,nbrOfShow):\n",
    "    outFile = open(\"out.csv\",\"w\")\n",
    "    csvwriter = csv.DictWriter(outFile,[\"position\",\"token\",\"tscore\",\"freqDropouts\",\"freqFinishers\"])\n",
    "    csvwriter.writeheader()\n",
    "    tscores1 = tscore.computeTscore(tscoreData1,tscoreData1reference)\n",
    "    tscores2 = tscore.computeTscore(tscoreData2,tscoreData2reference)\n",
    "    tscores1sorted = sorted(tscores1.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    tscores2sorted = sorted(tscores2.items(), key=operator.itemgetter(1),reverse=True)\n",
    "    combined = [ tscores1sorted[i]+tscores2sorted[i] for i in range(min(len(tscores1sorted),len(tscores2sorted))) ]\n",
    "    position = 0\n",
    "    for tuple in combined:\n",
    "        position += 1\n",
    "        (token1,token1Tscore,token2,token2Tscore) = tuple\n",
    "        if token1 in tscoreData1[WORDFREQS]: frequency11 = tscoreData1[WORDFREQS][token1]\n",
    "        else: frequency11 = 0\n",
    "        if token1 in tscoreData1reference[WORDFREQS]: frequency1r = tscoreData1reference[WORDFREQS][token1]\n",
    "        else: frequency1r = 0\n",
    "        if token2 in tscoreData2[WORDFREQS]: frequency22 = tscoreData2[WORDFREQS][token2]\n",
    "        else: frequency22 = 0\n",
    "        if token2 in tscoreData2reference[WORDFREQS]: frequency2r = tscoreData2reference[WORDFREQS][token2]\n",
    "        else: frequency2r = 0\n",
    "        csvwriter.writerow({\"position\":position,\"token\":token1,\"tscore\":token1Tscore,\\\n",
    "                            \"freqDropouts\":frequency11,\"freqFinishers\":frequency1r})\n",
    "        if position <= nbrOfShow: \n",
    "            print(\"{0:6d}. {2:5.1f} {3:6d} {4:6d} {1:<20s} {5:6d}. {7:5.1f} {8:6d} {9:6d} {6:<20s}\".\\\n",
    "                  format(position,token1,round(token1Tscore,1),frequency11,frequency1r,\\\n",
    "                         position,token2,round(token2Tscore,1),frequency22,frequency2r))\n",
    "    outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we choose the text collections that we want to compare. Here are the choices:\n",
    "\n",
    "1. all mails of all clients\n",
    "2. all mails of all counselors\n",
    "3. the final mail written by a counselor to a client that dropped out\n",
    "4. the final mail written to a counselor to a client that finished the therapy\n",
    "5. the first four mails of a client that dropped out\n",
    "6. the first four mails of a client that finished the therapy\n",
    "7. the long mails among the first four mails of a client that dropped out\n",
    "8. the long mails among the first four mails of a client that finished the therapy\n",
    "9. all mails of all clients that dropped out\n",
    "10. all mails of all clients that finished the therapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clientWithMails = [ clientId for clientId in allMails.keys() if len(allMails[clientId]) > 0 ][0]\n",
    "textFieldId = LIWCLIB.getFieldId(allMails[clientWithMails][0],FIELDNAMETEXT) \n",
    "\n",
    "def wordCount(text): return(len(text.split()))\n",
    "\n",
    "def selectMailsFrom(mails,sender):\n",
    "    return([ mail for mail in mails if mail[FIELDNAMEFROM] == sender ])\n",
    "\n",
    "def selectAllMailTextFrom(mails,sender):\n",
    "    return(\" \".join([ mail.metas[textFieldId] for mail in selectMailsFrom(mails,sender) ]))\n",
    "\n",
    "def selectLastNMailTextsFrom(mails,sender,n):\n",
    "    return(\" \".join([ mail.metas[textFieldId] for mail in selectMailsFrom(mails,sender) ][-n:]))\n",
    "\n",
    "def selectFirstNMailTextsFrom(mails,sender,n,cutoff=0):\n",
    "    firstNtexts = [ mail.metas[textFieldId] for mail in selectMailsFrom(mails,sender) ][0:n]\n",
    "    return(\" \".join([ text for text in firstNtexts if wordCount(text) >= cutoff ]))\n",
    "\n",
    "allMailsClients = [ selectAllMailTextFrom(allMails[thisId],CLIENT) for thisId in allMails ]\n",
    "allMailsCounselors = [ selectAllMailTextFrom(allMails[thisId],COUNSELOR) for thisId in allMails ]\n",
    "\n",
    "NBROFSELECTED = 1\n",
    "lastCounselorMails = { thisId:selectLastNMailTextsFrom(allMails[thisId],COUNSELOR,NBROFSELECTED) for thisId in allMails }\n",
    "lastCounselorMailsDropout = [ lastCounselorMails[clientId] for clientId in dropout if dropout[clientId] == CODEDROPOUT ]\n",
    "lastCounselorMailsFinisher = [ lastCounselorMails[clientId] for clientId in dropout if dropout[clientId] == CODEFINISHER ]\n",
    "\n",
    "NBROFSELECTED = 4\n",
    "firstFourClientMails = { thisId:selectFirstNMailTextsFrom(allMails[thisId],CLIENT,NBROFSELECTED) for thisId in allMails }\n",
    "firstFourClientMailsDropout = [ firstFourClientMails[clientId] for clientId in dropout if dropout[clientId] == CODEDROPOUT ]\n",
    "firstFourClientMailsFinisher = [ firstFourClientMails[clientId] for clientId in dropout if dropout[clientId] == CODEFINISHER ]\n",
    "\n",
    "CUTOFF = 1000\n",
    "NBROFSELECTED = 4\n",
    "firstFourClientMailsCutoff = { thisId:selectFirstNMailTextsFrom(allMails[thisId],CLIENT,NBROFSELECTED,cutoff=CUTOFF) for thisId in allMails }\n",
    "firstFourClientMailsCutoffDropout = [ firstFourClientMailsCutoff[clientId] for clientId in dropout if dropout[clientId] == CODEDROPOUT ]\n",
    "firstFourClientMailsCutoffFinisher = [ firstFourClientMailsCutoff[clientId] for clientId in dropout if dropout[clientId] == CODEFINISHER ]\n",
    "\n",
    "allMailsClientsDropout = [ selectAllMailTextFrom(allMails[clientId],CLIENT) for clientId in allMails if dropout[clientId] == CODEDROPOUT ]\n",
    "allMailsClientsFinisher = [ selectAllMailTextFrom(allMails[clientId],CLIENT) for clientId in allMails if dropout[clientId] == CODEFINISHER ]\n",
    "\n",
    "len(removeEmptyMails(allMailsClients)),len(removeEmptyMails(allMailsCounselors)),\\\n",
    "len(removeEmptyMails(lastCounselorMailsDropout)),len(removeEmptyMails(lastCounselorMailsFinisher)),\\\n",
    "len(removeEmptyMails(firstFourClientMailsDropout)),len(removeEmptyMails(firstFourClientMailsFinisher)),\\\n",
    "len(removeEmptyMails(firstFourClientMailsCutoffDropout)),len(removeEmptyMails(firstFourClientMailsCutoffFinisher)), \\\n",
    "len(removeEmptyMails(allMailsClientsDropout)),len(removeEmptyMails(allMailsClientsFinisher))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have selected different groups of text, we can use the t-score for comparing the word usage in our group with another. Note that the comparison results will be better for larger word groups. In general it does not makes sense to compare two small groups of texts because neither of the two will provide a good model of the language to compare the other with. In those cases it is better to compare a small text with a large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing all mail texts: dropouts vs finishers\n",
    "\n",
    "The two groups of all dropout mails and all finisher mails are large enough to compare with each other. However the results may not be very useful for two reasons. First, on average the finishers took part in the therapy for a longer time than the dropouts. So they participated more frequently in later assignments and the language used in these assignments will have an effect on the measurements. A second reasond reason for the results being less interesting is that we would like to detect likely dropouts as early as possible in the therapy, so we do not have the time to collect their responses during several weeks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFSHOW = 40\n",
    "\n",
    "print(\"counts:\",\"allMailsClientsDropout:\",sum([wordCount(text) for text in allMailsClientsDropout]),\"tokens;\",len(allMailsClientsDropout),\"mails;\", \\\n",
    "                \"allMailsClientsFinisher:\",sum([wordCount(text) for text in allMailsClientsFinisher]),\"tokens;\",len(allMailsClientsFinisher),\"mails\", )\n",
    "tscoreData1 = makeTscoreData(removeEmptyMails(allMailsClientsDropout))\n",
    "tscoreData2 = makeTscoreData(removeEmptyMails(allMailsClientsFinisher))\n",
    "compareTscoreData(tscoreData1,tscoreData2,tscoreData2,tscoreData1,NBROFSHOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that the finishers completed a larger part of the therapy is indeed reflected in their word usage, in particular by the therapy words *pols*, *vinger*, *actieplan*, *nameting*, *traject* and *opdracht*. The word *Dank* is also interesting here (displaying gratitude for a successful therapy?) as well as the two food-related diminuitives (*etentje* and *pilsje*: ability to relativate?). *gedachten*, *denken* and *herken* show the ability to reflect but may be invoked by later assignments. *trek* is an old-fashioned words correlated with the age of the client. The comma (*,*) was more used in this group (longer sentences?) while the periode (*.*) was more used by the dropouts (shorter sentences?).\n",
    "\n",
    "Interestingly, dropouts use more numbers (*NUM*) in their mails and more frequently refer to themselves (*mijn* and *Mijn*). The word *IK* is less interesting: it is an unusual spelling of a common word. There are seven more of these (*HET*, *DAT*, *VAN*, *DE*, *MIJN*, *EN* and *IS*) but they could be caused by a few clients writing all caps. There are many references to people and relations: *relatie*, *kinderen*, *huisarts*, *partner*, *ex*, *ouders*, *vrienden*, *contacten* and *vriendin*). *huisarts* could be used because of medical problems or extreme drinking behavior. While the finishers did not use negative words frequently, the dropouts did: *nadelen*, *problemen* and *klachten*. Two other interesting related words are *werk* and *hobby*, and there are also the three behavior-related words *drink*, *gebruik* and *stoppen*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDEFINED = \"UNDEF\"\n",
    "\n",
    "def makeCollocation(tokens,i,nbrOfTokensBefore,nbrOfTokensAfter):\n",
    "    collocationList = [tokens[i]]\n",
    "    for j in range(i-1,i-nbrOfTokensBefore-1,-1):\n",
    "        if j >= 0: collocationList = [tokens[j]]+collocationList\n",
    "        else: collocationList = [UNDEFINED]+collocationList\n",
    "    for j in range(i+1,i+nbrOfTokensAfter+1):\n",
    "        if j < len(tokens): collocationList.append(tokens[j])\n",
    "        else: collocationList.append(UNDEFINED)\n",
    "    return(\" \".join(collocationList))\n",
    "            \n",
    "def addToCollocations(collocations,tokens,i,nbrOfTokensBefore,nbrOfTokensAfter):\n",
    "    collocation = makeCollocation(tokens,i,nbrOfTokensBefore,nbrOfTokensAfter)\n",
    "    if collocation in collocations: collocations[collocation] += 1\n",
    "    else: collocations[collocation] = 1\n",
    "        \n",
    "def collocations(texts,token,nbrOfTokensBefore=2,nbrOfTokensAfter=3,nbrOfCollocations=20):\n",
    "    collocations = {}\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i in range(0,len(tokens)):\n",
    "            if tokens[i] == token: addToCollocations(collocations,tokens,i,nbrOfTokensBefore,nbrOfTokensAfter)\n",
    "    for key,value in sorted(collocations.items(), key=lambda item: item[1],reverse=True)[0:nbrOfCollocations]:\n",
    "        print(value,key)\n",
    "        \n",
    "collocations(allMailsClientsDropout,\"NUM\",nbrOfTokensBefore=3,nbrOfTokensAfter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUTID = 1\n",
    "FINISHERID = 2\n",
    "NBROFDROPOUTS = 437\n",
    "NBROFFINISHERS = 354\n",
    "\n",
    "len(mailTexts[DROPOUTID]) == NBROFDROPOUTS and len(mailTexts[FINISHERID]) == NBROFFINISHERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([wordCount(text) for text in allMailsClientsDropout])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of the list contains words that are more frequently used by the dropouts among the clients (category 1) than by the finishers (category 2), as can be seen from the counts in last two columns, for example for *mvg*: 16 > 3. \n",
    "\n",
    "Originally, this observation was not true for the top of the list, which contained mainly frequent words. Why was this the case? The problem proved to be the computation of the t-score. We removed the concepts of text lengths and vocabulary length used in the original definition of the t-score (Church, Gale, Hanks & Hindle, 1991) and replaced this with maximum attainable value: the number of clients per group, since the words were only counted once for each client. We kept using add-0.5 smoothing and adjusted the maximum attanable value with 0.5 as well to account for this. The top of the list improved a lot, with *mvg*, *hapje* and *late* occuring at the top of the list. The bottom of the list, words typically used by finishers did not contain many clear cases like *PS* on place -89 (0 vs 13) but no errors could be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTSIZE = 5\n",
    "dropoutLabel = 2\n",
    "searchWord = \"stress\"\n",
    "\n",
    "def printContext(text,word):\n",
    "    wordsInText = text.split()\n",
    "    for i in range(0,len(wordsInText)):\n",
    "        if wordsInText[i] == word:\n",
    "            for j in range(i-CONTEXTSIZE,i+CONTEXTSIZE+1):\n",
    "                try: print(wordsInText[j],end=\" \")\n",
    "                except: pass\n",
    "            print()    \n",
    "\n",
    "for clientId in mailTexts[dropoutLabel]:\n",
    "    if re.search(r\"\\b\"+searchWord+r\"\\b\",mailTexts[dropoutLabel][clientId]): \n",
    "        print(dropout[clientId],clientId)\n",
    "        printContext(mailTexts[dropoutLabel][clientId],searchWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check: male vs female words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "sys.path.append('/home/erikt/projects/e-mental-health/data-processing')\n",
    "import tactus2table\n",
    "\n",
    "DIRECTORY = \"/home/erikt/projects/e-mental-health/usb/tmp/20190917/\"\n",
    "FILENAMEPREFIX = \"^AdB\"\n",
    "TITLE = \"0-title\"\n",
    "FIELDNAMEINTAKE = \"Intake\"\n",
    "FIELDNAMEID = \"0-id\"\n",
    "FIELDNAMEGENDER = \"geslacht\"\n",
    "GZEXTENSION = \".gz\"\n",
    "XMLEXTENSION = \".xml\"\n",
    "MALE = \"man\"\n",
    "FEMALE = \"vrouw\"\n",
    "\n",
    "def shortenFileName(fileName):\n",
    "    return(re.sub(XMLEXTENSION,\"\",re.sub(GZEXTENSION,\"\",fileName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {}\n",
    "for inFileName in os.listdir(DIRECTORY):\n",
    "    if re.search(FILENAMEPREFIX,inFileName):\n",
    "        root = tactus2table.readRootFromFile(DIRECTORY+\"/\"+inFileName)\n",
    "        questionnaires = tactus2table.getQuestionnaires(root,inFileName)\n",
    "        for questionnaire in questionnaires: \n",
    "            if questionnaire[TITLE] == INTAKE:\n",
    "                for fieldName in questionnaire:\n",
    "                    if re.search(FIELDNAMEGENDER,fieldName):\n",
    "                        genders[shortenFileName(questionnaire[FIELDNAMEID])] = questionnaire[fieldName].lower()\n",
    "                        break\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderKeys = list(np.unique(np.array([genders[key] for key in genders])))\n",
    "genderMailTexts = {}\n",
    "for key in genderKeys: genderMailTexts[key] = {}\n",
    "for result in range(0,len(mailTexts)):\n",
    "    for fileId in mailTexts[result].keys():\n",
    "        genderMailTexts[genders[fileId]][fileId] = mailTexts[result][fileId]\n",
    "[(key,len(genderMailTexts[key])) for key in genderKeys]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscoreData1 = makeTscoreData(uniqueTextArray(genderMailTexts[MALE]))\n",
    "tscoreData2 = makeTscoreData(uniqueTextArray(genderMailTexts[FEMALE]))\n",
    "computeTscores(tscoreData1,tscoreData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-orange",
   "language": "python",
   "name": "python-orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
