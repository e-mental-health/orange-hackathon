{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a pipeline for comparing the vocabulary of two sets of Tactus emails with eachother by the t-score. The goal is to find tokens which appear more frequently in one set than in the other, and vice versa. This notebook uses much of the preprocessing of the notebook liwc.py in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code block specifies the required libraries. This includes some general Python libraries and some specific libraries developed in our research project. These project-specific libraries can be found in the folder orangehackathon/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../libs/\")\n",
    "import tactusloaderLIB\n",
    "import OWEmailSorterLIB\n",
    "import markduplicatesLIB\n",
    "import removemarkedtextLIB\n",
    "import LIWCLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block specifies the location of the therapy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"/home/erikt/projects/e-mental-health/usb/releases/20191217\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Python function was developed for storing the results of the data analysis (SaveResults). In Orange3 the module SaveData can be used for this task. (SaveResults might not be necessary for this notebook tscore.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULTOUTFILE=\"out.csv\"\n",
    "FIELDNAMEDATE = \"date\"\n",
    "FIELDNAMEFROM = \"from\"\n",
    "FIELDNAMEFILE = \"file\"\n",
    "FIELDNAMENBROFMAILS = \"nbr of mails\"\n",
    "CLIENT = \"CLIENT\"\n",
    "COUNSELOR = \"COUNSELOR\"\n",
    "FROMTARGET = CLIENT\n",
    "NBROFMATCHES = \"Number of matches\"\n",
    "\n",
    "# data selection settings\n",
    "PROCESSALLFEATURES = True\n",
    "AVERAGEROWS = False\n",
    "NBROFKEPTROWS = 4\n",
    "MINNBROFMATCHES = 50\n",
    "STUDENTFEATURENAMES = [FIELDNAMEFILE,FIELDNAMEFROM,FIELDNAMENBROFMAILS,\"4 i\",\"7 shehe\",\"8 they\",\"31 posemo\",\\\n",
    "                       \"32 negemo\",\"50 cogproc\",\"51 insight\",\"52 cause\",\"54 tentat\",\\\n",
    "                       \"90 focuspast\",\"91 focuspresent\",\"92 focusfuture\"]\n",
    "\n",
    "def addZero(string):\n",
    "    while len(string) < 2: string = \"0\"+string\n",
    "    return(string)\n",
    "\n",
    "def time2str(timeObj):\n",
    "    date = str(timeObj.tm_year)+\"-\"+addZero(str(timeObj.tm_mon))+\"-\"+addZero(str(timeObj.tm_mday))\n",
    "    time = addZero(str(timeObj.tm_hour))+\":\"+addZero(str(timeObj.tm_min))+\":\"+addZero(str(timeObj.tm_sec))\n",
    "    return(date+\" \"+time)\n",
    "\n",
    "def floatPrecision5(number):\n",
    "    if type(number) != type(0.5): return(number)\n",
    "    else: return(float(\"{0:.5f}\".format(number)))\n",
    "\n",
    "def saveResults(allLiwcResults,fileName=DEFAULTOUTFILE):\n",
    "    if len(allLiwcResults) > 0:\n",
    "        fieldNames = STUDENTFEATURENAMES\n",
    "        if PROCESSALLFEATURES:\n",
    "            fieldNames = [x.name for x in allLiwcResults[0].domain.variables]\n",
    "            fieldNames += [x.name for x in allLiwcResults[0].domain.metas]\n",
    "            fieldNames += [FIELDNAMENBROFMAILS]\n",
    "        outFile = open(fileName,\"w\")\n",
    "        with outFile as csvFile:\n",
    "            csvwriter = csv.DictWriter(csvFile,fieldnames=fieldNames)\n",
    "            csvwriter.writeheader()\n",
    "            for liwcResults in allLiwcResults:\n",
    "                if AVERAGEROWS:\n",
    "                    rowCounter = 0\n",
    "                    row = {}\n",
    "                    for liwcResultsRow in liwcResults:\n",
    "                        liwcResultsRow[FIELDNAMEFILE] = re.sub(\"-an.xml.gz\",\"\",str(liwcResultsRow[FIELDNAMEFILE]))\n",
    "                        if liwcResultsRow[FIELDNAMEFROM] == FROMTARGET:\n",
    "                            rowCounter += 1\n",
    "                            nbrOfMatches = 0\n",
    "                            if NBROFMATCHES in liwcResultsRow: nbrOfMatches = int(liwcResultsRow[NBROFMATCHES])\n",
    "                            if (NBROFKEPTROWS == 0 or rowCounter <= NBROFKEPTROWS) and \\\n",
    "                               (MINNBROFMATCHES == 0 or nbrOfMatches >= MINNBROFMATCHES):\n",
    "                                for fieldName in fieldNames:\n",
    "                                    if fieldName == FIELDNAMEDATE:\n",
    "                                        row[fieldName] = time2str(time.localtime(liwcResultsRow[fieldName].value))\n",
    "                                    elif not re.match(\"^\\d+\\s\",fieldName):\n",
    "                                        try: row[fieldName] = liwcResultsRow[fieldName].value\n",
    "                                        except: pass\n",
    "                                    elif fieldName in row: \n",
    "                                        row[fieldName] += floatPrecision5(liwcResultsRow[fieldName].value)\n",
    "                                    else: \n",
    "                                        row[fieldName] = floatPrecision5(liwcResultsRow[fieldName].value)\n",
    "                    if len(row) > 0:\n",
    "                        for fieldName in row:\n",
    "                            if re.match(\"^\\d+\\s\",fieldName) and rowCounter > 0: \n",
    "                                row[fieldName] = floatPrecision5(row[fieldName]/min(rowCounter,NBROFKEPTROWS))\n",
    "                        row[FIELDNAMENBROFMAILS] = rowCounter\n",
    "                        csvwriter.writerow(row)\n",
    "                else:\n",
    "                    rowCounter = 0\n",
    "                    row = {}\n",
    "                    for liwcResultsRow in liwcResults:\n",
    "                        liwcResultsRow[FIELDNAMEFILE] = re.sub(\"-an.xml.gz\",\"\",str(liwcResultsRow[FIELDNAMEFILE]))\n",
    "                        if liwcResultsRow[FIELDNAMEFROM] == FROMTARGET:\n",
    "                            rowCounter += 1\n",
    "                            nbrOfMatches = liwcResultsRow[NBROFMATCHES]\n",
    "                            if (NBROFKEPTROWS == 0 or rowCounter <= NBROFKEPTROWS) and \\\n",
    "                               (MINNBROFMATCHES == 0 or nbrOfMatches >= MINNBROFMATCHES):\n",
    "                                for fieldName in fieldNames:\n",
    "                                    if fieldName == FIELDNAMEDATE:\n",
    "                                        row[fieldName] = time2str(time.localtime(liwcResultsRow[fieldName].value))\n",
    "                                    elif not re.match(\"^\\d+\\s\",fieldName):\n",
    "                                        try: row[fieldName] = liwcResultsRow[fieldName].value\n",
    "                                        except: pass\n",
    "                                    else: \n",
    "                                        row[fieldName] = floatPrecision5(liwcResultsRow[fieldName].value)\n",
    "                                if len(row) > 0: csvwriter.writerow(row)\n",
    "        outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will comparethe texts in emails from clients that finished the treatment versus clients that dropped out. Thus we need the metadata which specifies the results of the therapy for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "\n",
    "DIRDROPOUT = \"/home/erikt/projects/e-mental-health/usb/releases/20200305/\"\n",
    "FILEDROPOUT = \"selected.csv.gz\"\n",
    "DELIMITER = \",\"\n",
    "FIELDNAMEDROPOUT = \"dropout\"\n",
    "FIELDNAMETEXT = \"text\"\n",
    "FIELDNAMEFILE = \"file\"\n",
    "\n",
    "dropout = {}\n",
    "inFile = gzip.open(DIRDROPOUT+FILEDROPOUT,\"rt\",encoding=\"utf-8\")\n",
    "csvreader = csv.DictReader(inFile,delimiter=DELIMITER)\n",
    "for row in csvreader: dropout[row[FIELDNAMEFILE]] = row[FIELDNAMEDROPOUT]\n",
    "inFile.close()\n",
    "\n",
    "len([x for x in dropout if dropout[x] != \"?\"]) == 791"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there is a loop which loads each available therapy file, runs the Orange3 pipeline. The Orange3 pipeline contains these parts:\n",
    "\n",
    "1. tactusloader: determine file name and read its contents\n",
    "2. sortMails: sort the mails from the file chronologically\n",
    "3. markduplicates: mark the parts of the mail text included from an earlier mail\n",
    "4. removemarkedtext: remove the marked text from the mail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 200 300 400 500 600 700 800 900 1000 1100 1200 1300 1400 1500 1600 1700 1800 1900 "
     ]
    }
   ],
   "source": [
    "MAXMAILS = 4\n",
    "\n",
    "allLiwcResults = []\n",
    "mailTexts = [[],[],[]]\n",
    "for patientId in list(range(1,1988)):\n",
    "    if patientId % 100 == 0: print(patientId,end=\" \")\n",
    "    fileName = tactusloaderLIB.makeFileName(str(patientId))\n",
    "    fileNameId = re.sub(\"-an.xml$\",\"\",fileName)\n",
    "    if fileNameId in dropout and (dropout[fileNameId] == \"1\" or dropout[fileNameId] == \"2\"):\n",
    "        mailText = \"\"\n",
    "        try:\n",
    "            mails = tactusloaderLIB.processFile(DIRECTORY,fileName+\".gz\")\n",
    "            if len(mails) > 0:\n",
    "                sortedMails = OWEmailSorterLIB.filterEmails(mails[0],filter_asc=True)\n",
    "                markedMails = markduplicatesLIB.processCorpus(sortedMails)\n",
    "                strippedMails = removemarkedtextLIB.processCorpus(markedMails)\n",
    "                mailCounter = 0\n",
    "                for strippedMail in strippedMails:\n",
    "                    if strippedMail[FIELDNAMEFROM] == CLIENT and mailCounter < MAXMAILS:\n",
    "                        mailText += str(strippedMail[FIELDNAMETEXT])\n",
    "                        mailCounter += 1\n",
    "        except:\n",
    "            print(\"problem processing file\",fileName)\n",
    "            continue\n",
    "        mailTexts[int(dropout[fileNameId])].append(mailText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "437\n",
      "354\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(mailTexts)): print(len(mailTexts[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the text to the data format of the t-score script: /home/erikt/projects/newsgac/fasttext-runs/tscore.py . There are two ways for computing the t-scores: count every separate word used by a client or count each word used by a client only once. The texts can be prepared for the second type of counts with the function uniqTextArray()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFTOKENS = \"totalFreq\"\n",
    "NBROFTYPES = \"nbrOfWords\"\n",
    "WORDFREQS = \"wordFreqs\"\n",
    "MAXCOUNT = \"maxCount\"\n",
    "\n",
    "def makeTscoreData(textArray):\n",
    "    data = { NBROFTOKENS:0, NBROFTYPES:0, MAXCOUNT:len(textArray), WORDFREQS:{} }\n",
    "    for text in textArray:\n",
    "        for token in text.split():\n",
    "            data[NBROFTOKENS] += 1\n",
    "            if token in data[WORDFREQS]: \n",
    "                data[WORDFREQS][token] += 1\n",
    "            else:\n",
    "                data[WORDFREQS][token] = 1\n",
    "                data[NBROFTYPES] += 1\n",
    "    return(data)\n",
    "\n",
    "def uniqText(text):\n",
    "    seen = {}\n",
    "    for word in text.split():\n",
    "        if not word in seen: seen[word] = True\n",
    "    return(\" \".join(list(seen.keys())))\n",
    "\n",
    "def uniqTextArray(textArrayIn):\n",
    "    textArrayOut = []\n",
    "    for text in textArrayIn:\n",
    "        textArrayOut.append(uniqText(text))\n",
    "    return(textArrayOut)\n",
    "\n",
    "tscoreData1 = makeTscoreData(uniqTextArray(mailTexts[1]))\n",
    "tscoreData2 = makeTscoreData(uniqTextArray(mailTexts[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 mvg 2.6069437444795414 16 3\n",
      "2 hapje 2.5902771425883144 8 0\n",
      "3 late 2.3803980606815736 39 16\n",
      "4 schoonouders 2.3313558655467217 12 2\n",
      "5 armen 2.2281041549259886 9 1\n",
      "6 externe 2.1831297072252833 6 0\n",
      "7 Vanmiddag 2.1831297072252833 6 0\n",
      "8 opgestoken 2.1831297072252833 6 0\n",
      "9 oppas 2.1831297072252833 6 0\n",
      "10 an 2.0246074095722575 8 1\n",
      "11 Mvg 2.0246074095722575 8 1\n",
      "12 groenten 1.960228533856725 10 2\n",
      "13 inhoudt 1.960228533856725 10 2\n",
      "14 langzamer 1.9513725557372972 5 0\n",
      "15 bios 1.9513725557372972 5 0\n",
      "16 uitwerking 1.9513725557372972 5 0\n",
      "17 verkouden 1.9513725557372972 5 0\n",
      "18 T 1.9513725557372972 5 0\n",
      "19 baalt 1.9513725557372972 5 0\n",
      "20 ondervinden 1.9513725557372972 5 0\n",
      "29871 sportschool -4.093160180780137 9 33\n",
      "29872 leeg -4.098771481280528 25 55\n",
      "29873 Een -4.101059978239603 103 143\n",
      "29874 lijkt -4.143443243577608 68 106\n",
      "29875 voordelen -4.14647358033523 136 178\n",
      "29876 hoofd -4.18232537098759 63 101\n",
      "29877 emotionele -4.202963434984855 28 60\n",
      "29878 groep -4.209407611770016 18 47\n",
      "29879 Dank -4.215909306760129 46 82\n",
      "29880 pijn -4.24257336063911 64 103\n",
      "29881 zeggen -4.288635576511674 118 162\n",
      "29882 bespreken -4.361528784773401 8 34\n",
      "29883 toch -4.36917264611581 207 254\n",
      "29884 gevoel -4.389052908745457 150 197\n",
      "29885 uur -4.390321253604521 149 196\n",
      "29886 voelde -4.516511327947278 71 115\n",
      "29887 hem -4.522499569798921 114 162\n",
      "29888 Wat -4.533226467080627 111 159\n",
      "29889 glas -4.620015788749803 74 120\n",
      "29890 gedachten -4.830482064818652 40 83\n"
     ]
    }
   ],
   "source": [
    "sys.path.append(\"/home/erikt/projects/newsgac/fasttext-runs\")\n",
    "import tscore\n",
    "import operator\n",
    "\n",
    "TOPPOS = 20\n",
    "\n",
    "outFile = open(\"out.csv\",\"w\")\n",
    "csvwriter = csv.DictWriter(outFile,[\"position\",\"token\",\"tscore\",\"freqDropouts\",\"freqFinishers\"])\n",
    "csvwriter.writeheader()\n",
    "tscores = tscore.computeTscoreList(tscoreData1,tscoreData2)\n",
    "position = 0\n",
    "for tuple in sorted(tscores.items(), key=operator.itemgetter(1),reverse=True):\n",
    "    position += 1\n",
    "    (token,tscore) = tuple\n",
    "    if token in tscoreData1[WORDFREQS]: frequency1 = tscoreData1[WORDFREQS][token]\n",
    "    else: frequency1 = 0\n",
    "    if token in tscoreData2[WORDFREQS]: frequency2 = tscoreData2[WORDFREQS][token]\n",
    "    else: frequency2 = 0\n",
    "    csvwriter.writerow({\"position\":position,\"token\":token,\"tscore\":tscore,\"freqDropouts\":frequency1,\"freqFinishers\":frequency2})\n",
    "    if position <= TOPPOS or position+TOPPOS > len(tscores): print(position,token,tscore,frequency1,frequency2)\n",
    "outFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottom of the list contains words that are more frequently used by the finishers among the clients (category 2) than by the dropouts (category 1). However, the reverse is not true for the top of the list, which contains mainly frequent words. Why is this the case? To give an example: whu is the tscore of *me* (frequencies 355 and 322) similar to that of *mvg* (16, 3)?\n",
    "\n",
    "The problem proved to be the computation of the t-score. We removed the concepts of text lengths and vocabulary length from the definition and replaced this with maximum attainable value: the number of clients per group, since the words were only counted once for each client. We kept using add-0.5 smoothing and adjusted the maximum attanable value with 0.5 as well to account for this. The top of the list improved a lot, with *mvg*, *hapje* and *late* occuring at the top of the list. The bottom of the list, words typically used by finishers did not contain many clear cases like *PS* on place -89 (0,13) but no errors could be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
