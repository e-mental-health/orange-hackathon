{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a pipeline for comparing the vocabulary of two sets of Tactus emails with eachother by the t-score. The goal is to find tokens which appear more frequently in one set than in the other, and vice versa. This notebook uses much of the preprocessing of the notebook liwc.py in this directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code block specifies the required libraries. This includes some general Python libraries and some specific libraries developed in our research project. These project-specific libraries can be found in the folder orangehackathon/libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "sys.path.append(\"../libs/\")\n",
    "import tactusloaderLIB\n",
    "import OWEmailSorterLIB\n",
    "import markduplicatesLIB\n",
    "import removemarkedtextLIB\n",
    "import LIWCLIB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code block specifies the location of the therapy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = \"/home/erikt/projects/e-mental-health/usb/releases/20191217\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Python function was developed for storing the results of the data analysis (SaveResults). In Orange3 the module SaveData can be used for this task. (SaveResults might not be necessary for this notebook tscore.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will comparethe texts in emails from clients that finished the treatment versus clients that dropped out. Thus we need the metadata which specifies the results of the therapy for each client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "DIRDROPOUT = \"/home/erikt/projects/e-mental-health/usb/releases/20200305/\"\n",
    "FILEDROPOUT = \"selected.csv.gz\"\n",
    "DELIMITER = \",\"\n",
    "FIELDNAMEDROPOUT = \"dropout\"\n",
    "FIELDNAMETEXT = \"text\"\n",
    "FIELDNAMEFILE = \"file\"\n",
    "FIELDNAMEFROM = \"from\"\n",
    "CLIENT = \"CLIENT\"\n",
    "NBROFCLIENTS = 791\n",
    "\n",
    "dropout = {}\n",
    "inFile = gzip.open(DIRDROPOUT+FILEDROPOUT,\"rt\",encoding=\"utf-8\")\n",
    "csvreader = csv.DictReader(inFile,delimiter=DELIMITER)\n",
    "for row in csvreader: dropout[row[FIELDNAMEFILE]] = row[FIELDNAMEDROPOUT]\n",
    "inFile.close()\n",
    "\n",
    "len([x for x in dropout if dropout[x] != \"?\"]) == NBROFCLIENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally there is a loop which loads each available therapy file, runs the Orange3 pipeline. The Orange3 pipeline contains these parts:\n",
    "\n",
    "1. tactusloader: determine file name and read its contents\n",
    "2. sortMails: sort the mails from the file chronologically\n",
    "3. markduplicates: mark the parts of the mail text included from an earlier mail\n",
    "4. removemarkedtext: remove the marked text from the mail\n",
    "\n",
    "The file loading takes some time. The program counts from 0 to 1987 in steps of 100 to indicate its progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXMAILS = 4\n",
    "MAXCLIENT = 1987\n",
    "CUTOFF = 1000\n",
    "\n",
    "def wordCount(text): return(len(text.split()))\n",
    "\n",
    "allLiwcResults = []\n",
    "mailTexts = [{},{},{}]\n",
    "print(\"(0..\"+str(MAXCLIENT)+\"):\",0,end=\" \")\n",
    "for patientId in range(1,MAXCLIENT+1):\n",
    "    if patientId % 100 == 0: print(patientId,end=\" \")\n",
    "    fileName = tactusloaderLIB.makeFileName(str(patientId))\n",
    "    fileNameId = re.sub(\"-an.xml$\",\"\",fileName)\n",
    "    if fileNameId in dropout and (dropout[fileNameId] == \"1\" or dropout[fileNameId] == \"2\"):\n",
    "        mailText = \"\"\n",
    "        try:\n",
    "            mails = tactusloaderLIB.processFile(DIRECTORY,fileName+\".gz\")\n",
    "            if len(mails) > 0:\n",
    "                sortedMails = OWEmailSorterLIB.filterEmails(mails[0],filter_asc=True)\n",
    "                markedMails = markduplicatesLIB.processCorpus(sortedMails)\n",
    "                strippedMails = removemarkedtextLIB.processCorpus(markedMails)\n",
    "                mailCounter = 0\n",
    "                for strippedMail in strippedMails:\n",
    "                    if strippedMail[FIELDNAMEFROM] == CLIENT and mailCounter < MAXMAILS:\n",
    "                        mailTextPerMail = str(strippedMail[FIELDNAMETEXT])\n",
    "                        mailCounter += 1\n",
    "                        if wordCount(mailTextPerMail) >= CUTOFF: \n",
    "                            mailText += mailTextPerMail+\" \"\n",
    "        except:\n",
    "            print(\"problem processing file\",fileName)\n",
    "            continue\n",
    "        mailTexts[int(dropout[fileNameId])][fileNameId] = mailText\n",
    "print(patientId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUTID = 1\n",
    "FINISHERID = 2\n",
    "NBROFDROPOUTS = 437\n",
    "NBROFFINISHERS = 354\n",
    "\n",
    "len(mailTexts[DROPOUTID]) == NBROFDROPOUTS and len(mailTexts[FINISHERID]) == NBROFFINISHERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the text to the data format of the t-score script: /home/erikt/projects/newsgac/fasttext-runs/tscore.py . There are two ways for computing the t-scores: count every separate word used by a client or count each word used by a client only once. The texts can be prepared for the second type of counts with the function uniqueTextArray() which removes all duplicate words from the texts (case-sensitive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBROFTOKENS = \"totalFreq\"\n",
    "NBROFTYPES = \"nbrOfWords\"\n",
    "WORDFREQS = \"wordFreqs\"\n",
    "MAXCOUNT = \"maxCount\"\n",
    "\n",
    "def removeEmptyMails(mails):\n",
    "    clientsWithoutMails = [ clientId for clientId in mails if mails[clientId] == \"\" ]\n",
    "    mailsCopy = dict(mails)\n",
    "    for clientId in clientsWithoutMails:\n",
    "        del(mailsCopy[clientId])\n",
    "    return(mailsCopy)\n",
    "        \n",
    "def makeTscoreData(textArray):\n",
    "    data = { NBROFTOKENS:0, NBROFTYPES:0, MAXCOUNT:len(textArray), WORDFREQS:{} }\n",
    "    for text in textArray:\n",
    "        for token in text.split():\n",
    "            data[NBROFTOKENS] += 1\n",
    "            if token in data[WORDFREQS]: \n",
    "                data[WORDFREQS][token] += 1\n",
    "            else:\n",
    "                data[WORDFREQS][token] = 1\n",
    "                data[NBROFTYPES] += 1\n",
    "    return(data)\n",
    "\n",
    "def uniqueText(text):\n",
    "    seen = {}\n",
    "    for word in text.split():\n",
    "        if not word in seen: seen[word] = True\n",
    "    return(\" \".join(list(seen.keys())))\n",
    "\n",
    "def uniqueTextArray(textDictIn):\n",
    "    textArrayOut = []\n",
    "    for fileNameId in textDictIn:\n",
    "        textArrayOut.append(uniqueText(textDictIn[fileNameId]))\n",
    "    return(textArrayOut)\n",
    "\n",
    "def normalizeMaxCount(tscoreData,fraction):\n",
    "    tscoreData[MAXCOUNT] = round(tscoreData[MAXCOUNT]*fraction,1)\n",
    "    for word in tscoreData[\"wordFreqs\"]:\n",
    "        tscoreData[\"wordFreqs\"][word] = round(tscoreData[\"wordFreqs\"][word]*fraction,1)\n",
    "    return(tscoreData)\n",
    "    \n",
    "tscoreData1 = makeTscoreData(uniqueTextArray(removeEmptyMails(mailTexts[1])))\n",
    "tscoreData2 = makeTscoreData(uniqueTextArray(removeEmptyMails(mailTexts[2])))\n",
    "# tscoreData2 = normalizeMaxCount(tscoreData2,tscoreData1[\"maxCount\"]/tscoreData2[\"maxCount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/erikt/projects/newsgac/fasttext-runs\")\n",
    "import tscore\n",
    "import operator\n",
    "\n",
    "TOPPOS = 20\n",
    "\n",
    "def computeTscores(tscoreData1,tscoreData2):\n",
    "    outFile = open(\"out.csv\",\"w\")\n",
    "    csvwriter = csv.DictWriter(outFile,[\"position\",\"token\",\"tscore\",\"freqDropouts\",\"freqFinishers\"])\n",
    "    csvwriter.writeheader()\n",
    "    tscores = tscore.computeTscoreUniqueWords(tscoreData1,tscoreData2)\n",
    "    position = 0\n",
    "    for tuple in sorted(tscores.items(), key=operator.itemgetter(1),reverse=True):\n",
    "        position += 1\n",
    "        (token,tokenTscore) = tuple\n",
    "        if token in tscoreData1[WORDFREQS]: frequency1 = tscoreData1[WORDFREQS][token]\n",
    "        else: frequency1 = 0\n",
    "        if token in tscoreData2[WORDFREQS]: frequency2 = tscoreData2[WORDFREQS][token]\n",
    "        else: frequency2 = 0\n",
    "        csvwriter.writerow({\"position\":position,\"token\":token,\"tscore\":tokenTscore,\\\n",
    "                            \"freqDropouts\":frequency1,\"freqFinishers\":frequency2})\n",
    "        if position <= TOPPOS or position+TOPPOS > len(tscores): \n",
    "            print(position,token,tokenTscore,frequency1,frequency2)\n",
    "    outFile.close()\n",
    "    \n",
    "computeTscores(tscoreData1,tscoreData2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top of the list contains words that are more frequently used by the dropouts among the clients (category 1) than by the finishers (category 2), as can be seen from the counts in last two columns, for example for *mvg*: 16 > 3. \n",
    "\n",
    "Originally, this observation was not true for the top of the list, which contained mainly frequent words. Why was this the case? The problem proved to be the computation of the t-score. We removed the concepts of text lengths and vocabulary length used in the original definition of the t-score (Church, Gale, Hanks & Hindle, 1991) and replaced this with maximum attainable value: the number of clients per group, since the words were only counted once for each client. We kept using add-0.5 smoothing and adjusted the maximum attanable value with 0.5 as well to account for this. The top of the list improved a lot, with *mvg*, *hapje* and *late* occuring at the top of the list. The bottom of the list, words typically used by finishers did not contain many clear cases like *PS* on place -89 (0 vs 13) but no errors could be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXTSIZE = 5\n",
    "dropoutLabel = 2\n",
    "searchWord = \"stress\"\n",
    "\n",
    "def printContext(text,word):\n",
    "    wordsInText = text.split()\n",
    "    for i in range(0,len(wordsInText)):\n",
    "        if wordsInText[i] == word:\n",
    "            for j in range(i-CONTEXTSIZE,i+CONTEXTSIZE+1):\n",
    "                try: print(wordsInText[j],end=\" \")\n",
    "                except: pass\n",
    "            print()    \n",
    "\n",
    "for clientId in mailTexts[dropoutLabel]:\n",
    "    if re.search(r\"\\b\"+searchWord+r\"\\b\",mailTexts[dropoutLabel][clientId]): \n",
    "        print(dropout[clientId],clientId)\n",
    "        printContext(mailTexts[dropoutLabel][clientId],searchWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check: male vs female words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "sys.path.append('/home/erikt/projects/e-mental-health/data-processing')\n",
    "import tactus2table\n",
    "\n",
    "DIRECTORY = \"/home/erikt/projects/e-mental-health/usb/tmp/20190917/\"\n",
    "FILENAMEPREFIX = \"^AdB\"\n",
    "TITLE = \"0-title\"\n",
    "FIELDNAMEINTAKE = \"Intake\"\n",
    "FIELDNAMEID = \"0-id\"\n",
    "FIELDNAMEGENDER = \"geslacht\"\n",
    "GZEXTENSION = \".gz\"\n",
    "XMLEXTENSION = \".xml\"\n",
    "MALE = \"man\"\n",
    "FEMALE = \"vrouw\"\n",
    "\n",
    "def shortenFileName(fileName):\n",
    "    return(re.sub(XMLEXTENSION,\"\",re.sub(GZEXTENSION,\"\",fileName)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = {}\n",
    "for inFileName in os.listdir(DIRECTORY):\n",
    "    if re.search(FILENAMEPREFIX,inFileName):\n",
    "        root = tactus2table.readRootFromFile(DIRECTORY+\"/\"+inFileName)\n",
    "        questionnaires = tactus2table.getQuestionnaires(root,inFileName)\n",
    "        for questionnaire in questionnaires: \n",
    "            if questionnaire[TITLE] == INTAKE:\n",
    "                for fieldName in questionnaire:\n",
    "                    if re.search(FIELDNAMEGENDER,fieldName):\n",
    "                        genders[shortenFileName(questionnaire[FIELDNAMEID])] = questionnaire[fieldName].lower()\n",
    "                        break\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genderKeys = list(np.unique(np.array([genders[key] for key in genders])))\n",
    "genderMailTexts = {}\n",
    "for key in genderKeys: genderMailTexts[key] = {}\n",
    "for result in range(0,len(mailTexts)):\n",
    "    for fileId in mailTexts[result].keys():\n",
    "        genderMailTexts[genders[fileId]][fileId] = mailTexts[result][fileId]\n",
    "[(key,len(genderMailTexts[key])) for key in genderKeys]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscoreData1 = makeTscoreData(uniqueTextArray(genderMailTexts[MALE]))\n",
    "tscoreData2 = makeTscoreData(uniqueTextArray(genderMailTexts[FEMALE]))\n",
    "computeTscores(tscoreData1,tscoreData2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-orange",
   "language": "python",
   "name": "python-orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
